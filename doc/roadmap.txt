================================================================================
Browser Search + Docs Reader MCP Server (Rust + Brave + SQLite)        *mcp-web*
================================================================================
A. Product Spec: What the server exposes                                    *A*
================================================================================
A1. MCP Capabilities
--------------------------------------------------------------------------------
- Tools:
  - web_search
  - web_open
  - web_batch_open
  - web_extract
  - cache_get
  - cache_purge
- Resources:
  - resource://cache/<sha256>        => the cached Markdown for a doc snapshot
  - resource://meta/<sha256>         => fetch metadata (headers, timings, etc.)
- Prompts:
  - docs_summarize_for_coding
  - docs_extract_api_surface

MCP servers are designed to expose tools/resources/prompts to hosts/clients.


A2. Tool Contracts (opinionated, minimal, sharp)
--------------------------------------------------------------------------------
See doc/schema.txt for complete input/output schemas.

Summary of tools:
(1) web_search       - Brave API search with filtering/pagination
(2) web_open         - Fetch + extract URL (raw/readable/rendered modes)
(3) web_batch_open   - Batch fetch with concurrency control
(4) web_extract      - Extract from provided HTML (no network)
(5) cache_get        - Retrieve cached snapshot by hash
(6) cache_purge      - Purge cache entries by age/domain/count

================================================================================
B. Architecture & Modules (Rust)                                           *B*
================================================================================
B1. Crate layout (workspace)
--------------------------------------------------------------------------------
mcp-web/
  Cargo.toml (workspace)
  crates/
    server                   ; (crate) main server binary + tool wiring
    client                   ; (crate) client code shared by server + CLI
      brave                  ; (mod) Brave API client
      fetch                  ; (mod) HTTP fetch, robots, SSRF protections
      extract                ; (mod) Lectito adapter + markdown normalization
        adapter              ; (mod) thin wrapper around lectito-core
        normalize            ; (mod) markdown normalization + frontmatter
        links                ; (mod) link harvesting / URL fixing
      render                 ; (mod) headless browser renderer
    core                     ; (crate) shared structs (serde), errors, config
      cache                  ; (mod) SQLite cache + migrations
    cli                      ; (crate) CLI binary


B2. Key dependencies
--------------------------------------------------------------------------------
- MCP: official Rust SDK rmcp + macros for tools.
- HTTP: reqwest + rustls
- Extraction:
  - lectito-core (Readability.js-inspired extraction engine)
    - default: disable "fetch" feature in server builds
    - keep "markdown" + "siteconfig" features enabled
- SQLite:
  - tokio_rusqlite (async rustqlite bindings)
- Robots:
  - robotstxt crate (or equivalent) (respect robots by default)
- "rendered" mode:
  - rust-headless-chrome (CDP) or chromiumoxide
    - rust-headless-chrome is a high-level DevTools Protocol wrapper.

Cargo.toml (server crate) suggestion:
  rmcp -> features = ["server"]
  lectito-core -> features = ["markdown","siteconfig"]
  reqwest -> ["rustls-tls","gzip","brotli","deflate"]
  tokio_rusqlite
  tokio -> ["rt-multi-thread","macros"]
  serde -> ["derive"]

B3. Config surface (env + config file)
--------------------------------------------------------------------------------
- BRAVE_API_KEY (required for web_search)
- MCP_WEB_DB_PATH (default: ./mcp-web-cache.sqlite)
- MCP_WEB_USER_AGENT (default: mcp-web/0.x)
- MCP_WEB_MAX_BYTES (default: 5MB)
- MCP_WEB_TIMEOUT_MS (default: 20000)
- MCP_WEB_RESPECT_ROBOTS (default: true)
- MCP_WEB_RENDER_ENABLED (default: false)
- MCP_WEB_ALLOWLIST_DOMAINS (optional)
- MCP_WEB_DENYLIST_DOMAINS (optional)

================================================================================
C. SQLite Cache Spec (schema + invariants)                                 *C*
================================================================================

See doc/schema.txt for complete SQL schemas and cache invariants.

Summary:
- Content-addressed cache: hash = sha256(normalized_url + vary_headers + mode)
- Two tables: snapshots (document cache) + search_cache (search results)
- Revalidation via ETag/Last-Modified or TTL-based expiry
- Extractor metadata stored for reproducibility (lectito version, siteconfig)
- WAL mode, NORMAL synchronous, LRU-ish purge policy

================================================================================
D. Brave Search Client Spec                                                *D*
================================================================================

D1. Endpoint + auth
--------------------------------------------------------------------------------
- Base: https://api.search.brave.com/res/v1/
- Web Search endpoint: GET /web/search?q=...
- Auth: X-Subscription-Token header.

D2. Parameters to support
--------------------------------------------------------------------------------
- q (required)
- count (<=20; default 20 in docs)
- offset (0..9 page offset)
- freshness (pd|pw|pm|py|custom range)
- safesearch (off|moderate|strict; default moderate)
- country, search_lang, ui_lang
- extra_snippets=true
- goggles
- enable_rich_callback=1

D3. Result normalization
--------------------------------------------------------------------------------
Normalize Brave’s response into a stable internal struct:
  SearchResult { title, url, description, extra_snippets, rank }

D4. Rate limiting and retries
- Respect Brave’s published rate limiting docs (implement token bucket).
- Retry only on:
  - 429 with backoff
  - transient 5xx
- Never retry non-idempotent endpoints (all ours are GET).

================================================================================
E. Fetch Pipeline Spec (raw HTML fetch)                                    *E*
================================================================================

E1. URL canonicalization
--------------------------------------------------------------------------------
- Trim whitespace, ensure scheme
- Default scheme: https
- Normalize:
  - lowercase host
  - remove fragment (#...)
  - keep query string intact (do not reorder unless you’re brave)

E2. SSRF + safety gates (default ON)
--------------------------------------------------------------------------------
- Deny:
  - file://, data://, ftp://, chrome:// etc.
  - localhost, 127.0.0.0/8, ::1
  - RFC1918 private ranges (10/8, 172.16/12, 192.168/16)
  - link-local (169.254/16), multicast, etc.
- Resolve DNS and validate all A/AAAA answers are public.
- Max redirects: 5
- Max body bytes: configurable (default 5MB)
- Timeout: configurable

E3. robots.txt compliance
--------------------------------------------------------------------------------
- Fetch robots.txt per host (cache it for 24h)
- Evaluate user-agent group:
  - Use "*" and your UA
- If disallowed:
  - Return a structured error:
    { code: "ROBOTS_DISALLOWED", url, path, robots_url }

(Robotstxt crate is commonly suggested for this purpose.)

================================================================================
F. Readable Extraction Spec                                                  *F*
================================================================================

F1. Primary algorithm
--------------------------------------------------------------------------------
- Use Lectito's extraction pipeline (Readability.js-inspired):
  - preprocessing (remove scripts/styles/comments/unlikely candidates)
  - scoring (tag/class/id patterns, density, link density)
  - best-candidate selection + sibling inclusion
  - cleanup (empty nodes, relative URL fixes, formatting rules)

F2. Server-facing extraction API (stable abstraction)
--------------------------------------------------------------------------------
Define an internal trait so you can swap engines later without touching tools:

  trait Extractor {
    fn extract(&self, html: &str, base_url: &str, cfg: ExtractConfig)
      -> Result<ExtractedDoc>;
  }

Implement Extractor via Lectito:
- Document::parse(html)
- extract_content(&doc, &lectito_core::ExtractConfig)
- doc.extract_metadata()
- convert_to_markdown(extracted.content, metadata, markdown_cfg)

(These APIs are shown in Lectito's README examples.)

F3. Site configs (first-class now)
--------------------------------------------------------------------------------
- Wire MCP "web_open(mode=readable)" to optionally apply site configs:
  - Config lookup order:
    1. explicit tool argument {siteconfig: "..."} (advanced usage)
    2. domain-matched rules in ./site_configs/
    3. no site rules (default)
- Store "siteconfig id/version used" in cache metadata to keep outputs
  reproducible.

Lectito supports optional XPath-based extraction rules for difficult sites.

F4. Output normalization (still required)
--------------------------------------------------------------------------------
Even if Lectito emits markdown, enforce a consistent header:

  ---
  title: ...
  source: ...
  fetched_at: ...
  extractor: lectito-core@<version>
  siteconfig: <id or none>
  ---
  <markdown>

Keep this stable so your agent can cite/compare docs over time.

F5. Performance budget
--------------------------------------------------------------------------------
- readable mode:
  - fetch <= 2s typical
  - extract <= 200ms typical on commodity hardware
- Any single tool call should time out before it blocks the agent loop.

================================================================================
G. Rendered Mode Spec (Headless browser)                                   *G*
================================================================================
This is feature-gated and off by default.

G1. When to use
--------------------------------------------------------------------------------
- If content-type is text/html but:
  - body is tiny and script-heavy
  - extractor yields < N chars
  - page is known SPA doc site (config heuristics)

G2. Implementation options
--------------------------------------------------------------------------------
- rust-headless-chrome: high-level CDP control, Puppeteer-like.
- chromiumoxide: async CDP alternative.

G3. Render flow
--------------------------------------------------------------------------------
- Launch with a shared browser instance (pool)
- Navigate -> wait:
  - DOMContentLoaded + network idle heuristic
- Extract:
  - document.documentElement.outerHTML (for readability)
  - document.body.innerText (for fallback)
- Feed HTML to extraction pipeline (F.*)
- Cache snapshot with mode=rendered.

G4. Hard safety limits
--------------------------------------------------------------------------------
- Max render time: 15s
- Disable downloads
- Block popups
- Optional: block third-party requests by default (configurable allowlist)


================================================================================
H. MCP Server Implementation Plan (rmcp)                                   *H*
================================================================================

H1. Server skeleton
--------------------------------------------------------------------------------
- Use rmcp + #[tool] macro to declare tools; route through ToolRouter.
- Transport: stdio
- Runtime: tokio

H2. Tool wiring
--------------------------------------------------------------------------------
- web_search -> brave-client -> normalize -> optional short TTL cache
- web_open -> cache lookup -> fetch -> extract -> cache upsert
- web_batch_open -> bounded concurrency w/ tokio semaphore
- web_extract -> pure function over html text (no network)
- cache_get/cache_purge -> cache crate

H3. Error model (uniform)
--------------------------------------------------------------------------------
Return structured errors (thiserror) that the agent can reason about:
- INVALID_URL
- SSRF_BLOCKED
- ROBOTS_DISALLOWED
- FETCH_TIMEOUT
- FETCH_TOO_LARGE
- HTTP_ERROR (with status_code)
- BRAVE_AUTH_ERROR
- BRAVE_RATE_LIMITED
- EXTRACT_FAILED
- RENDER_DISABLED
- RENDER_FAILED
- CACHE_ERROR

H4. Observability
--------------------------------------------------------------------------------
- Log in JSON lines (stderr):
  { ts, tool, url, ms, cache_hit, status, error_code? }
- Add per-tool counters:
  - cache_hit_rate
  - avg_fetch_ms / extract_ms / render_ms


================================================================================
I. Security & Compliance Checklist                                         *I*
================================================================================

I1. Security
--------------------------------------------------------------------------------
- SSRF protections (E2)
- robots.txt respect (E3)
- sanitize URLs in logs (no leaking tokens)
- cap bytes + timeouts everywhere
- limit concurrency and parallelism
- disable rendered mode unless explicitly enabled

I2. Content rights
--------------------------------------------------------------------------------
- Store only what you need:
  - extracted text/markdown is usually enough
  - avoid storing full raw HTML unless required
Brave notes they provide ranked public URLs/snippets; content rights remain with
publishers—your access must comply with publisher terms.


================================================================================
J. Test Plan                                                                 *J*
================================================================================
J1. Unit tests
--------------------------------------------------------------------------------
- URL normalization (E1)
- SSRF blocklist (E2): ensure private ranges denied
- Cache hashing stability (C1)
- Brave request builder (D2) (no real network)
- Extraction:
  - small fixture pages (docs, blog, SPA shell)
  - ensures code blocks preserved

J2. Integration tests (feature-gated)
--------------------------------------------------------------------------------
- Use VCR-style recorded HTTP fixtures (never hit Brave in CI by default)
- SQLite migrations apply cleanly
- web_open returns cached results on second call
- robots disallow scenario using a local test server

================================================================================
K. Milestones                                                                *K*
================================================================================

K1. Milestone 1: MCP stdio server (no network)
--------------------------------------------------------------------------------
- [x] rmcp server boots on stdio (tools registered via #[tool])
- [x] Implement web_extract via Lectito (Document::parse + extract_content)
      (Use git url: https://github.com/stormlightlabs/lectito)

K2. Milestone 2: SQLite cache + migrations
--------------------------------------------------------------------------------
- [ ] Implement cache mod with tokio_rustqlite
- [ ] Add migrations for snapshots/search_cache (C2)
- [ ] Enable WAL + pragmas (C3)
- [ ] cache_get + cache_purge tools

K3. Milestone 3: HTTP fetch pipeline (safe) + readable mode
--------------------------------------------------------------------------------
- [ ] Implement SSRF checks + redirect limits + max_bytes (E2)
- [ ] Implement robots.txt fetch + decision cache (E3)
- [ ] Implement web_open(readable):
      fetch HTML -> Lectito extract -> markdown normalize -> cache

K4. Milestone 4: Brave web_search tool
--------------------------------------------------------------------------------
- [ ] Implement brave-client:
      - headers/auth
      - q/count/offset/freshness/safesearch/country/lang/extra_snippets
- [ ] Normalize results + return more_results_available where possible
- [ ] Add short TTL search_cache table usage (optional)
Refs: Brave web search docs for pagination, safesearch, freshness, snippets.

K5. Milestone 5: Site-config pipeline (Lectito advantage)
--------------------------------------------------------------------------------
- [ ] Add ./site_configs/ domain mapping
- [ ] Add cache metadata for siteconfig + extractor version
- [ ] Add a "debug: explain_extraction" output to help tuning
      (e.g., which candidate won, how many siblings included, etc.)
- [ ] web_batch_open with bounded concurrency
- [ ] Add "domain_allowlist" post-filter for search results (A2)
- [ ] Add stable markdown header + link harvest (F4)

K6. Milestone 6: Rendered mode (optional feature)
--------------------------------------------------------------------------------
- [ ] Add render crate (rust-headless-chrome or chromiumoxide)
- [ ] Render HTML via headless browser -> feed HTML to Lectito
- [ ] This keeps *one* extraction engine across raw vs rendered flows.
Refs: headless Chrome control options.

K7. Milestone 7: Hardening + observability
--------------------------------------------------------------------------------
- [ ] JSONL logs + metrics counters (H4)
- [ ] Fuzz-ish tests for URL parsing inputs
- [ ] Cache purge policy (C4) + size ceiling
- [ ] Add documentation: SECURITY.md + threat model

K8. Milestone 8: Packaging + integration with your agent harness
--------------------------------------------------------------------------------
- [ ] Provide a single binary release
- [ ] Provide example MCP config for your TUI harness (stdio invocation)
- [ ] Add "recipes":
      - search -> open top3 -> summarize pipeline
      - API docs extraction pattern

================================================================================
L. Reference URLs                                                          *L*
================================================================================
MCP (Model Context Protocol)
- https://modelcontextprotocol.io/specification/2025-06-18/basic/transports
- https://modelcontextprotocol.io/docs/learn/architecture
- https://modelcontextprotocol.io/docs/develop/build-server
- https://modelcontextprotocol.io/specification/2025-06-18/server/prompts
- https://modelcontextprotocol.io/specification/2025-06-18/server/resources

rmcp (official Rust MCP SDK)
- https://github.com/modelcontextprotocol/rust-sdk
- https://docs.rs/rmcp

Brave Search API
- https://api-dashboard.search.brave.com/documentation/quickstart
- https://api-dashboard.search.brave.com/app/documentation/web-search/get-started
- https://api-dashboard.search.brave.com/api-reference/web/search
- https://api-dashboard.search.brave.com/terms-of-service

Readable extraction (Rust)
- https://github.com/stormlightlabs/lectito

SQLite (Rust)
- https://docs.rs/tokio-rusqlite/latest/tokio_rusqlite/

Headless browser (Rust)
- https://crates.io/crates/headless_chrome
- https://docs.rs/chromiumoxide/latest/chromiumoxide/
- https://crates.io/crates/chromiumoxide

robots.txt parsing (Rust)
- https://docs.rs/crate/robotstxt-rs/latest
- https://github.com/Folyd/robotstxt
